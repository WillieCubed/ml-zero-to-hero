{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Zero to Hero: MNIST",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrNQiNVngynW",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_xu1pkYg1ge",
        "colab_type": "text"
      },
      "source": [
        "# ML Zero to Hero: MNIST\n",
        "I want to rebuild my confidence in programming and finally get started on my ML project. This is the first mini-project on my path to building really cool stuff.\n",
        "\n",
        "This mini-project is literally stolen from the \"Getting started with TensorFlow 2.0\" guide, but I had to start somewhere."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ec2eD5C0hKys",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBjDNqbRhTTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the MNIST from TensorFlow's built-in datasets\n",
        "mnist = tf.keras.datasets.mnist\n",
        "# Separate features and labels into train and test data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMHuC49ahvV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an artificial neural network\n",
        "model = tf.keras.models.Sequential([\n",
        "    \n",
        "    # Flatten the image into a 1 * 28 * 28 vector\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    \n",
        "    # Use a fully-connected layer of 128 perceptrons,\n",
        "    # each using a ReLU activation function\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    \n",
        "    # Dropout/\"forget\" 20% of the output neurons to prevent overfitting\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    \n",
        "    # Use another fully-connected layer of 10 perceptrons (output),\n",
        "    # each with a softmax activation function that \n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Build the model\n",
        "model.compile(\n",
        "    # An optimizer that unlike gradient descent dynamically changes the\n",
        "    # learning rate during training\n",
        "    # More info: https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\n",
        "    optimizer='adam',\n",
        "    \n",
        "    # A form of cross entropy for mutually-exclusive classes\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "\n",
        "    # Print out the accuracy during training\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWt-paEoj5YV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "c862e46d-98b6-4189-faf6-326a61023699"
      },
      "source": [
        "# Train the model with the training data\n",
        "# Go over training data five times (5 epochs)\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Test the model with the test data\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 5s 82us/sample - loss: 2.5671 - acc: 0.7587\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 4s 71us/sample - loss: 0.5956 - acc: 0.8500\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 4s 72us/sample - loss: 0.4882 - acc: 0.8769\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 4s 71us/sample - loss: 0.4202 - acc: 0.8924\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 4s 70us/sample - loss: 0.3868 - acc: 0.9019\n",
            "10000/10000 [==============================] - 0s 36us/sample - loss: 0.3077 - acc: 0.9350\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3076707109142095, 0.935]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKDK14J24fKE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "7962ff07-aaf9-4f69-af21-0080953c05c2"
      },
      "source": [
        "# Okay, technically the tutorial says we're done, but\n",
        "# I'm going further\n",
        "\n",
        "# Train and test the model using 10 epochs\n",
        "model.fit(x_train, y_train, epochs=10)\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 4s 71us/sample - loss: 0.3303 - acc: 0.9154\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 4s 72us/sample - loss: 0.3264 - acc: 0.9190\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 4s 71us/sample - loss: 0.3193 - acc: 0.9204\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 4s 70us/sample - loss: 0.3165 - acc: 0.9214\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 4s 72us/sample - loss: 0.3042 - acc: 0.9232\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 4s 72us/sample - loss: 0.3060 - acc: 0.9236\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 4s 71us/sample - loss: 0.3018 - acc: 0.9249\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 4s 72us/sample - loss: 0.2991 - acc: 0.9259\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 4s 69us/sample - loss: 0.3015 - acc: 0.9246\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 4s 69us/sample - loss: 0.2939 - acc: 0.9278\n",
            "10000/10000 [==============================] - 0s 34us/sample - loss: 0.3122 - acc: 0.9485\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3121532320836559, 0.9485]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABQrxcX45AA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now use more layers\n",
        "model2 = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model2.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZSO4rnf8Cm1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "7a1c40a2-807d-4475-9b34-81af6e0e50df"
      },
      "source": [
        "model2.fit(x_train, y_train, epochs=20)\n",
        "model2.evaluate(x_test, y_test)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 2.6546 - acc: 0.1752\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 2.1042 - acc: 0.1885\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 5s 83us/sample - loss: 2.0805 - acc: 0.1939\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 5s 83us/sample - loss: 2.0368 - acc: 0.2019\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 5s 83us/sample - loss: 2.0246 - acc: 0.2054\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 5s 83us/sample - loss: 1.9388 - acc: 0.2438\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 5s 83us/sample - loss: 1.6988 - acc: 0.3304\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 5s 83us/sample - loss: 1.5401 - acc: 0.4059\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 5s 83us/sample - loss: 1.4575 - acc: 0.4236\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 5s 85us/sample - loss: 1.3537 - acc: 0.4719\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 5s 83us/sample - loss: 1.2065 - acc: 0.5571\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 5s 82us/sample - loss: 1.0422 - acc: 0.6067\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 5s 81us/sample - loss: 0.9337 - acc: 0.6579\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 5s 82us/sample - loss: 0.7691 - acc: 0.7482\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 5s 84us/sample - loss: 0.6462 - acc: 0.7896\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 5s 84us/sample - loss: 0.5332 - acc: 0.8300\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.4676 - acc: 0.8528\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 5s 85us/sample - loss: 0.4203 - acc: 0.8691\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 5s 83us/sample - loss: 0.3741 - acc: 0.8902\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 5s 81us/sample - loss: 0.3397 - acc: 0.9052\n",
            "10000/10000 [==============================] - 0s 42us/sample - loss: 0.2282 - acc: 0.9414\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.22822360603250563, 0.9414]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    }
  ]
}